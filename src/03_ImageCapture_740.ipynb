{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ifcopenshell, ifcopenshell.geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifc = ifcopenshell.open(\"../data/IfcOpenHouse_IFC4.ifc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display House \n",
    "\n",
    "found technique in /pythonocc-demos/examples/ifc_clip_plane.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:OCC.Display.backend:backend loaded: qt-pyqt5\n",
      "INFO:OCC.Display.SimpleGui:GUI backend set to: qt-pyqt5\n",
      " ###### 3D rendering pipe initialisation #####\n",
      "Display3d class initialization starting ...\n",
      "Aspect_DisplayConnection created.\n",
      "Graphic_Driver created.\n",
      "V3d_Viewer created.\n",
      "AIS_InteractiveContext created.\n",
      "V3d_View created\n",
      "Xw_Window created.\n",
      "Display3d class successfully initialized.\n",
      " ########################################\n"
     ]
    }
   ],
   "source": [
    "from OCC.Core.Quantity import Quantity_Color, Quantity_TOC_RGB\n",
    "from OCC.Display.SimpleGui import init_display\n",
    "\n",
    "\n",
    "# Specify to return pythonOCC shapes from ifcopenshell.geom.create_shape()\n",
    "settings = ifcopenshell.geom.settings()\n",
    "settings.set(settings.USE_PYTHON_OPENCASCADE, True)\n",
    "\n",
    "display, start_display, add_menu, add_function_to_menu = init_display()\n",
    "\n",
    "products = ifc.by_type(\"IfcProduct\")\n",
    "for product in products:\n",
    "    if product.is_a(\"IfcOpeningElement\"): continue\n",
    "    if product.Representation:\n",
    "        shape = ifcopenshell.geom.create_shape(settings, product)\n",
    "        r, g, b, a = shape.styles[0] # the shape color\n",
    "        color = Quantity_Color(abs(r), abs(g), abs(b), Quantity_TOC_RGB)\n",
    "        display_shape = display.DisplayShape(shape.geometry,color=color, transparency=abs(1 -a),update=True)\n",
    "\n",
    "start_display() #must be used or kernel crashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can we control the camera?\n",
    "\n",
    "found in /pythonocc-demos/examples/core_visualization_camera.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29.167191557828815, 0.25919333745672435, 11.136959669569574)\n"
     ]
    }
   ],
   "source": [
    "cam = display.View.Camera()  # type: Graphic3d_Camera\n",
    "\n",
    "center = cam.Center()\n",
    "eye = cam.Eye()\n",
    "print(eye.XYZ().Coord())\n",
    "\n",
    "eye.SetY(eye.Y() + 0.5)\n",
    "cam.SetEye(eye)\n",
    "\n",
    "#What is eye? What is center?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's figure out where what this 'cam' is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'OCC.Core.Graphic3d' from '/home/alon/LocalInstalls/miniconda3/envs/pyoccenv740/lib/python3.8/site-packages/OCC/Core/Graphic3d.py'>\n",
      "<class 'OCC.Core.Graphic3d.Graphic3d_Camera'>\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getmodule(cam))\n",
    "print(type(cam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module file is one step above 'swig' which interfaces between python and cpp code (occ,oce... pythonocc wraps them)\n",
    "\n",
    "The class 'Graphic3d_Camera' inherits a-lot from swig\n",
    "\n",
    "We can find things like this:</br> \n",
    "   Center = _swig_new_instance_method(_Graphic3d.Graphic3d_Camera_Center)</br>\n",
    "    ConvertProj2View = _swig_new_instance_method(_Graphic3d.Graphic3d_Camera_ConvertProj2View) </br>\n",
    "    ConvertView2Proj = _swig_new_instance_method(_Graphic3d.Graphic3d_Camera_ConvertView2Proj)</br>\n",
    "    ConvertView2World = _swig_new_instance_method(_Graphic3d.Graphic3d_Camera_ConvertView2World)</br>\n",
    "    ConvertWorld2View = _swig_new_instance_method(_Graphic3d.Graphic3d_Camera_ConvertWorld2View)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not so useful... we need reading material\n",
    "lucky, we found this https://github.com/tpaviot/pythonocc-core/issues/860 </br>\n",
    "which points to this https://dev.opencascade.org/doc/occt-7.4.0/overview/html/occt_user_guides__visualization.html#occt_visu_4_3 </br>\n",
    "I also found this https://dev.opencascade.org/doc/refman/html/class_graphic3d___camera.html\n",
    "\n",
    "There it is said: </br>\n",
    "Eye – defines the observer (camera) position. Make sure the Eye point never gets between the Front and Back clipping planes. </br>\n",
    "Center – defines the origin of View Reference Coordinates (where camera is aimed at). </br>\n",
    "Direction – defines the direction of camera view (from the Eye to the Center). </br>\n",
    "Distance – defines the distance between the Eye and the Center. </br>\n",
    "Front Plane – defines the position of the front clipping plane in View Reference Coordinates system. </br>\n",
    "Back Plane – defines the position of the back clipping plane in View Reference Coordinates system. </br>\n",
    "ZNear – defines the distance between the Eye and the Front plane. </br>\n",
    "ZFar – defines the distance between the Eye and the Back plane. </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cam.Znear() = 14.631771225017076\n",
      "cam.ZFar() = 48.89027396526461\n",
      "cam.Center().XYZ().Coord() = (0.695345168250018, 0.950588122094497, -1.4184640672483262)\n",
      "cam.Distance() = 31.117861976787673\n",
      "cam.Eye().XYZ().Coord() = (29.167191557828815, 0.7591933374567243, 11.136959669569574)\n",
      "cam.FOVy() = 45.0\n",
      "cam.Up().Coord() = (-0.3601352271800932, 0.40764382866614407, 0.8391240236665556)\n",
      "cam.OrientationMatrix().GetData = <Swig Object of type 'double *' at 0x7ff892b0ae10>\n",
      "cam.Aspect = 1.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print('cam.Znear() = {}'.format(cam.ZNear()))\n",
    "print('cam.ZFar() = {}'.format(cam.ZFar()))\n",
    "print('cam.Center().XYZ().Coord() = {}'.format(cam.Center().XYZ().Coord()))\n",
    "print('cam.Distance() = {}'.format(cam.Distance()))\n",
    "print('cam.Eye().XYZ().Coord() = {}'.format(cam.Eye().XYZ().Coord()))\n",
    "print('cam.FOVy() = {}'.format(cam.FOVy())) #FOVx doesnt exist. documentation says its automatically computed by aspect ratio\n",
    "print('cam.Up().Coord() = {}'.format(cam.Up().Coord()))\n",
    "print('cam.OrientationMatrix().GetData = {}'.format(cam.OrientationMatrix().GetData()))\n",
    "print('cam.Aspect = {}'.format(cam.Aspect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like its all roses until we try and get what we really need... what is this swig stuff again. </br>\n",
    "\n",
    "prehaps this hold some information: https://stackoverflow.com/questions/51776809/read-swig-python-double-object-into-numpy-maybe-through-ctypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of GetData():\n",
      "<Swig Object of type 'double *' at 0x7ff892b0ae70>\n",
      "\n",
      "after conversion, M = \n",
      "[[ 1.69640774e-01  9.13098407e-01 -3.70773930e-01 -1.51186889e+00]\n",
      " [-3.66136119e-01  4.07692861e-01  8.36499177e-01  1.05358702e+00]\n",
      " [ 9.14967950e-01 -6.15064058e-03  4.03479640e-01 -3.11759124e+01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "\n",
      "Camera's eye location calcualted from matrix:\n",
      "[29.16719156  0.75919334 11.13695967]\n",
      "Camera's eye location from object:\n",
      "(29.167191557828815, 0.7591933374567243, 11.136959669569574)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ctypes import c_double\n",
    "from ctypes import c_float\n",
    "\n",
    "data = cam.OrientationMatrix().GetData()\n",
    "print('output of GetData():')\n",
    "print(data, end = '\\n\\n')\n",
    "v = list((c_double * 16).from_address(int(data)))\n",
    "M = np.array(v).reshape((4,4)).T\n",
    "print('after conversion, M = ')\n",
    "print(M,end = '\\n\\n')\n",
    "\n",
    "R_w2c = M[0:3,0:3]\n",
    "R_c2w = R_w2c.T\n",
    "t_w_c2w = M[0:3,3]\n",
    "\n",
    "t_w_w2c = R_c2w @ (-t_w_c2w)\n",
    "print(\"Camera's eye location calcualted from matrix:\")\n",
    "print(t_w_w2c)\n",
    "print(\"Camera's eye location from object:\")\n",
    "print(cam.Eye().XYZ().Coord())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we can make some pretty pictures, we need to play with \"the constructs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Make a Transform\n",
    "\n",
    "from OCC.Core import gp\n",
    "\n",
    "def gpMat3toNumpy(gpMat3):\n",
    "    #gp_mat class is a 3x3 matrix class with no method to return values\n",
    "    #notice how the Row method accepts integers starting 1\n",
    "    row0 = gpMat3.Row(1).Coord()\n",
    "    row1 = gpMat3.Row(2).Coord()\n",
    "    row2 = gpMat3.Row(3).Coord()\n",
    "    M = np.array([row0,\n",
    "            row1,\n",
    "            row2])\n",
    "    return M\n",
    "\n",
    "#Lets Test it:\n",
    "gpT = gp.gp_Trsf() #initalize unit transform\n",
    "gpR = gpT.GetRotation().GetMatrix() #getRotation returns a quaternion representation\n",
    "npR = gpMat3toNumpy(gpR)\n",
    "print(npR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#Edit a Transform\n",
    "p = gp.gp_Pnt(0,0,1)\n",
    "v = gp.gp_Dir(0,0,1)\n",
    "ax1 = gp.gp_Ax1(p,v)\n",
    "\n",
    "gpT.SetRotation(ax1,np.pi/2)\n",
    "gpR = gpT.GetRotation().GetMatrix() #getRotation returns a quaternion representation\n",
    "npR = gpMat3toNumpy(gpR)\n",
    "print(npR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678 -0.70710678  0.        ]\n",
      " [ 0.70710678  0.70710678  0.        ]\n",
      " [ 0.          0.          1.        ]]\n",
      "(-0.0, 5.0, -0.0)\n"
     ]
    }
   ],
   "source": [
    "def vNtheta2Quat(v,theta):\n",
    "    #builds quaternion for rotation around vector v\n",
    "    #v - unit direction vector\n",
    "    #theta - angle in radians\n",
    "    qx = v[0] * np.sin(theta/2)\n",
    "    qy = v[1] * np.sin(theta/2)\n",
    "    qz = v[2] * np.sin(theta/2)\n",
    "    qw = np.cos(theta/2)\n",
    "    return (qx,qy,qz,qw)\n",
    "\n",
    "def makeTrsf(v,theta,t):\n",
    "    #builds transform such that the vector representation V_O1 will become VO2:\n",
    "    #V_O2 = T*V_O1\n",
    "    \n",
    "    #X_k+1 = T*X_k\n",
    "    #This transform was made to move things around in the world space\n",
    "\n",
    "    #v - unit direction vector\n",
    "    #theta - angle in radians with which to rotate O1 until it has the same orientation as O2\n",
    "    #t translation in O1 coordinates to O2\n",
    "\n",
    "    gpT = gp.gp_Trsf() #initalize unit transform\n",
    "    q = vNtheta2Quat(v,-theta)\n",
    "    gpq = gp.gp_Quaternion(q[0],q[1],q[2],q[3])\n",
    "    gpT.SetRotation(gpq)\n",
    "\n",
    "    R = gpMat3toNumpy(gpq.GetMatrix())\n",
    "    t = -R @ np.array(t)\n",
    "    gpT.SetTranslationPart(gp.gp_Vec(t[0],t[1],t[2]))\n",
    "    return gpT\n",
    "    \n",
    "\n",
    "#Some Testing\n",
    "T = makeTrsf(v = (0,0,1), theta = np.pi/4, t = (0, 5, 0))\n",
    "T.Invert() #we want to evolve, not to convert coordiante system\n",
    "print(gpMat3toNumpy(T.GetRotation().GetMatrix()))\n",
    "print(T.TranslationPart().Coord())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that was exhusting.. we need to build a proper numpy wrapper for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alright lets set up some pretty pictures\n",
    "\n",
    "IFC-Demos/ref-gits/pythonocc-demos/examples/...</br>\n",
    "core_offscreen_rendering.py </br>\n",
    "core_display_export_to_image.py </br>\n",
    "\n",
    "for later debug: https://github.com/tpaviot/pythonocc-core/issues/526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ###### 3D rendering pipe initialisation #####\n",
      "Display3d class initialization starting ...\n",
      "Aspect_DisplayConnection created.\n",
      "Graphic_Driver created.\n",
      "V3d_Viewer created.\n",
      "AIS_InteractiveContext created.\n",
      "V3d_View created\n",
      "Display3d class successfully initialized.\n",
      " ########################################\n",
      " ###### 3D rendering pipe initialisation #####\n",
      "Display3d class initialization starting ...\n",
      "Aspect_DisplayConnection created.\n",
      "Graphic_Driver created.\n",
      "V3d_Viewer created.\n",
      "AIS_InteractiveContext created.\n",
      "V3d_View created\n",
      "Display3d class successfully initialized.\n",
      " ########################################\n"
     ]
    }
   ],
   "source": [
    "from OCC.Display.OCCViewer import OffscreenRenderer\n",
    "offscreen_renderer = OffscreenRenderer()\n",
    "offscreen_renderer.Create()\n",
    "offscreen_renderer.SetModeShaded()\n",
    "\n",
    "products = ifc.by_type(\"IfcProduct\")\n",
    "for product in products:\n",
    "    if product.is_a(\"IfcOpeningElement\"): continue\n",
    "    if product.Representation:\n",
    "        shape = ifcopenshell.geom.create_shape(settings, product)\n",
    "        r, g, b, a = shape.styles[0] # the shape color\n",
    "        color = Quantity_Color(abs(r), abs(g), abs(b), Quantity_TOC_RGB)\n",
    "        display_shape = offscreen_renderer.DisplayShape(shape.geometry,color=color, transparency=abs(1 -a),update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = makeTrsf(v = (1,0,0), theta = -np.pi/4, t =(5,5,10))\n",
    "T.Invert() #again.. we want to evolve\n",
    "\n",
    "cam = offscreen_renderer.View.Camera()\n",
    "cam.Transform(T)\n",
    "\n",
    "from OCC.Core.Graphic3d import Graphic3d_Camera\n",
    "cam.SetProjectionType(Graphic3d_Camera.Projection_Perspective)\n",
    "\n",
    "offscreen_renderer.View.Dump('./capture_jpeg.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OCC.Core.Graphic3d import Graphic3d_BufferType\n",
    "# export to a 640*480 image data\n",
    "data_640_480 = offscreen_renderer.GetImageData(640, 480, Graphic3d_BufferType.Graphic3d_BT_Depth)\n",
    "\n",
    "# the same image to 1024*768\n",
    "data_1024_768 = offscreen_renderer.GetImageData(1024, 768, Graphic3d_BufferType.Graphic3d_BT_Depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bac7bbf8055d1db8b3b6da9e074f63b1cfe78cf401c996d2ba9e506790babe57"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
